{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "k1gpzj4guo8e1riwj3om1k",
        "id": "zhffIbD8yRi8"
      },
      "source": [
        "## N-gram language models\n",
        "\n",
        "Данные - корпус [ArXiv](http://arxiv.org/) статей с [kaggle](https://www.kaggle.com/neelshah18/arxivdataset/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellId": "u8jdaiy68oib3jvr4k01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXrK77kdyRjA",
        "outputId": "3678ed8b-eefd-4558-9e53-126792cb9a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk import WordPunctTokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellId": "0c76vnyl3zui9yhtkodgrlf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "ghDyWzC1yRjA",
        "outputId": "46a90124-645a-4285-a493-968b3cbd08d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-25 20:02:18--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.7.18, 2620:100:6027:18::a27d:4812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.7.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz [following]\n",
            "--2023-03-25 20:02:18--  https://www.dropbox.com/s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc3fb1927d0b9debd2cb1bc3015b.dl.dropboxusercontent.com/cd/0/get/B46uSfY5hzYGKYdxjTBEfmYW-B_Dy6TWxtzKEkphLSJnW25HhC9xGFTOFxkVLvdjthdjqZo5flZfcBeriXO-TcxqlwFqAJZoFSQc_j-quDxY6Qx3lwhz4eQhlsf2l5fyQHcTMc-pW1a49F7DzoLAQ5IhXwu_JAqmQ0x7KMEiyvQ7fA/file?dl=1# [following]\n",
            "--2023-03-25 20:02:18--  https://uc3fb1927d0b9debd2cb1bc3015b.dl.dropboxusercontent.com/cd/0/get/B46uSfY5hzYGKYdxjTBEfmYW-B_Dy6TWxtzKEkphLSJnW25HhC9xGFTOFxkVLvdjthdjqZo5flZfcBeriXO-TcxqlwFqAJZoFSQc_j-quDxY6Qx3lwhz4eQhlsf2l5fyQHcTMc-pW1a49F7DzoLAQ5IhXwu_JAqmQ0x7KMEiyvQ7fA/file?dl=1\n",
            "Resolving uc3fb1927d0b9debd2cb1bc3015b.dl.dropboxusercontent.com (uc3fb1927d0b9debd2cb1bc3015b.dl.dropboxusercontent.com)... 162.125.7.15, 2620:100:6027:15::a27d:480f\n",
            "Connecting to uc3fb1927d0b9debd2cb1bc3015b.dl.dropboxusercontent.com (uc3fb1927d0b9debd2cb1bc3015b.dl.dropboxusercontent.com)|162.125.7.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18933283 (18M) [application/binary]\n",
            "Saving to: ‘arxivData.json.tar.gz’\n",
            "\n",
            "arxivData.json.tar. 100%[===================>]  18.06M  70.4MB/s    in 0.3s    \n",
            "\n",
            "2023-03-25 20:02:19 (70.4 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n",
            "\n",
            "arxivData.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  author  day            id  \\\n",
              "24203  [{'name': 'Sandip Rakshit'}, {'name': 'Subhadi...   30   1003.5891v1   \n",
              "4195   [{'name': 'David Bau'}, {'name': 'Bolei Zhou'}...   19  1704.05796v1   \n",
              "26532  [{'name': 'Hyeonseob Nam'}, {'name': 'Jung-Woo...    2  1611.00471v2   \n",
              "29661  [{'name': 'Helen L. Bear'}, {'name': 'Gari Owe...    3  1710.01084v1   \n",
              "36999                   [{'name': 'Jorge Martinez-Gil'}]   23  1602.07064v1   \n",
              "\n",
              "                                                    link  month  \\\n",
              "24203  [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
              "4195   [{'rel': 'alternate', 'href': 'http://arxiv.or...      4   \n",
              "26532  [{'rel': 'alternate', 'href': 'http://arxiv.or...     11   \n",
              "29661  [{'rel': 'alternate', 'href': 'http://arxiv.or...     10   \n",
              "36999  [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
              "\n",
              "                                                 summary  \\\n",
              "24203  In the present work, we have used Tesseract 2....   \n",
              "4195   We propose a general framework called Network ...   \n",
              "26532  We propose Dual Attention Networks (DANs) whic...   \n",
              "29661  In the quest for greater computer lip-reading ...   \n",
              "36999  In this work we present SIFT, a 3-step algorit...   \n",
              "\n",
              "                                                     tag  \\\n",
              "24203  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "4195   [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "26532  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "29661  [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
              "36999  [{'term': 'cs.DB', 'scheme': 'http://arxiv.org...   \n",
              "\n",
              "                                                   title  year  \n",
              "24203  Recognition of Handwritten Roman Script Using ...  2010  \n",
              "4195   Network Dissection: Quantifying Interpretabili...  2017  \n",
              "26532  Dual Attention Networks for Multimodal Reasoni...  2016  \n",
              "29661  Some observations on computer lip-reading: mov...  2017  \n",
              "36999  SIFT: An Algorithm for Extracting Structural I...  2016  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7436e059-e948-485b-a429-fa6f5aa96726\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24203</th>\n",
              "      <td>[{'name': 'Sandip Rakshit'}, {'name': 'Subhadi...</td>\n",
              "      <td>30</td>\n",
              "      <td>1003.5891v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>3</td>\n",
              "      <td>In the present work, we have used Tesseract 2....</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Recognition of Handwritten Roman Script Using ...</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4195</th>\n",
              "      <td>[{'name': 'David Bau'}, {'name': 'Bolei Zhou'}...</td>\n",
              "      <td>19</td>\n",
              "      <td>1704.05796v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>4</td>\n",
              "      <td>We propose a general framework called Network ...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Network Dissection: Quantifying Interpretabili...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26532</th>\n",
              "      <td>[{'name': 'Hyeonseob Nam'}, {'name': 'Jung-Woo...</td>\n",
              "      <td>2</td>\n",
              "      <td>1611.00471v2</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>11</td>\n",
              "      <td>We propose Dual Attention Networks (DANs) whic...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Dual Attention Networks for Multimodal Reasoni...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29661</th>\n",
              "      <td>[{'name': 'Helen L. Bear'}, {'name': 'Gari Owe...</td>\n",
              "      <td>3</td>\n",
              "      <td>1710.01084v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>10</td>\n",
              "      <td>In the quest for greater computer lip-reading ...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Some observations on computer lip-reading: mov...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36999</th>\n",
              "      <td>[{'name': 'Jorge Martinez-Gil'}]</td>\n",
              "      <td>23</td>\n",
              "      <td>1602.07064v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>2</td>\n",
              "      <td>In this work we present SIFT, a 3-step algorit...</td>\n",
              "      <td>[{'term': 'cs.DB', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>SIFT: An Algorithm for Extracting Structural I...</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7436e059-e948-485b-a429-fa6f5aa96726')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7436e059-e948-485b-a429-fa6f5aa96726 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7436e059-e948-485b-a429-fa6f5aa96726');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
        "!tar -xvzf arxivData.json.tar.gz\n",
        "data = pd.read_json(\"./arxivData.json\")\n",
        "data.sample(n=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellId": "lbyqb5rx7j8jpo591r06ak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHYSvT1MyRjB",
        "outputId": "92cf4162-7076-4a81-fdb3-331f71028f83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
              " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
              " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "lines = data.apply(lambda x: x['title'] + ' ; ' + x['summary'].replace('\\n', ' '), axis=1).tolist()\n",
        "sorted(lines, key=len)[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "7u97m5s8ekl5zd5a43a1yc",
        "id": "Yr2UQjcCyRjC"
      },
      "source": [
        "### Токенизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellId": "u8rvfk719iek97t3rarwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh_SdCC8yRjC",
        "outputId": "6ef9b1e7-2424-488e-b3f1-edf361f33a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dual recurrent attention units for visual question answering ; we propose an architecture for vqa which utilizes recurrent layers to generate visual and textual attention . the memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question . our single model outperforms the first place winner on the vqa 1 . 0 dataset , performs within margin to the current state - of - the - art ensemble model . we also experiment with replacing attention mechanisms in other state - of - the - art models with our implementation and show increased accuracy . in both cases , our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the vqa dataset .\n"
          ]
        }
      ],
      "source": [
        "tokenizer = WordPunctTokenizer()\n",
        "lines = [' '.join(tokenizer.tokenize(line.lower())) for line in lines]\n",
        "print(lines[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellId": "w88nddpp2k8edoeyyyjh0l",
        "id": "qI4zrG4ryRjC"
      },
      "outputs": [],
      "source": [
        "assert sorted(lines, key=len)[0] == 'differential contrastive divergence ; this paper has been retracted .'\n",
        "assert sorted(lines, key=len)[2] == 'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "qb6h3hxmr095egzv8rlzul",
        "id": "OWMGUr2OyRjD"
      },
      "source": [
        "### N-Gram Language Model\n",
        "\n",
        "Language model (языковая модель) - вероятностная модель, оценивающая вероятность текста:\n",
        "\n",
        "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$ \n",
        "\n",
        "Фактически вероятность последнего токена равна $P(w_T \\mid w_1, \\dots, w_{T-1})$ и зависит от $n-1$ предыдущих токенов, что очень неудобно считать на практике.\n",
        "\n",
        "Популярная апроксимация - предположить, что вероятность последнего токена зависит только от некоторого конечного числа предыдущих (n-gram model):\n",
        "\n",
        "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "u68wydbiioqlp5gl96mhd",
        "id": "CSYubNFVyRjD"
      },
      "source": [
        "Создадим модель, которая считает встречаемости токенов при условии предыдущих n-1 токенов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellId": "og84gjipnumsakhiiu9ap",
        "id": "CBhyenmKyRjD"
      },
      "outputs": [],
      "source": [
        "# - unk  - Unkwnown\n",
        "# - eos - end of sentence\n",
        "\n",
        "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
        "\n",
        "def count_ngrams(lines, n):\n",
        "    \"\"\"\n",
        "    Count how many times each word occured after (n - 1) previous words\n",
        "    :param lines: an iterable of strings with space-separated tokens\n",
        "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
        "    \"\"\"\n",
        "    counts = defaultdict(Counter)\n",
        "    for line in lines:\n",
        "      unks = UNK + ' '\n",
        "      unks = unks * (n - 1)\n",
        "      line = f'{unks} ' + line + f' {EOS}' # empty prefix: \"\" -> (UNK, UNK)\n",
        "      tokens = line.split()\n",
        "      for i in range(n - 1, len(tokens)):\n",
        "        words = tokens[i - (n - 1) : i]\n",
        "        word3 = tokens[i]\n",
        "        counts[tuple(words)][word3] += 1\n",
        "    return counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellId": "xyf2he6lak9mmqarl3nck",
        "id": "TwQzMZamyRjD"
      },
      "outputs": [],
      "source": [
        "dummy_lines = sorted(lines, key=len)[:100]\n",
        "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
        "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
        "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
        "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
        "assert dummy_counts['p', '=']['np'] == 2\n",
        "assert dummy_counts['author', '.']['_EOS_'] == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "4j620npeqvj0k8ak8xqx8xk",
        "id": "bnHDuTBAyRjE"
      },
      "source": [
        "Теперь мы можем построить вероятностную n-gram модель:\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellId": "c7cm76wmzlaa12bctznzei",
        "id": "WXjs12xdyRjE"
      },
      "outputs": [],
      "source": [
        "class NGramLanguageModel:    \n",
        "    def __init__(self, lines, n):\n",
        "        \"\"\" \n",
        "        Train a simple count-based language model: \n",
        "        compute probabilities P(w_t | prefix) given ngram counts\n",
        "        \n",
        "        :param n: computes probability of next token given (n - 1) previous words\n",
        "        :param lines: an iterable of strings with space-separated tokens\n",
        "        \"\"\"\n",
        "        assert n >= 1\n",
        "        self.n = n\n",
        "    \n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        self.probs = defaultdict(Counter)\n",
        "        for prefix, token_counts in counts.items():\n",
        "          sum_counts = sum(token_counts.values())\n",
        "          self.probs[(prefix)] = {\n",
        "              word3: token_count / sum_counts \n",
        "              for word3, token_count in token_counts.items()\n",
        "          } # any problems? \n",
        "            \n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
        "        \"\"\"\n",
        "        prefix = prefix.split()\n",
        "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
        "        prefix = [UNK] * (self.n - 1 - len(prefix)) + prefix\n",
        "        return self.probs[tuple(prefix)]\n",
        "    \n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :param next_token: the next token to predict probability for\n",
        "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
        "        \"\"\"\n",
        "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "0ftnn4nmuzrup6c0vvhb8q",
        "id": "OjFd95UsyRjE"
      },
      "source": [
        "Потестим"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellId": "a7zajcnvhqupvcrmacvkur",
        "id": "Q-r4uqdvyRjF"
      },
      "outputs": [],
      "source": [
        "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
        "\n",
        "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
        "assert np.allclose(p_initial['learning'], 0.02)\n",
        "assert np.allclose(p_initial['a'], 0.13)\n",
        "assert np.allclose(p_initial.get('meow', 0), 0)\n",
        "assert np.allclose(sum(p_initial.values()), 1)\n",
        "\n",
        "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
        "assert np.allclose(p_a['machine'], 0.15384615)\n",
        "assert np.allclose(p_a['note'], 0.23076923)\n",
        "assert np.allclose(p_a.get('the', 0), 0)\n",
        "assert np.allclose(sum(p_a.values()), 1)\n",
        "\n",
        "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
        "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
        "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
        "    \"your 3-gram model should only depend on 2 previous words\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "oh8r9a41kuk4r51wra9",
        "id": "QqnDvFchyRjF"
      },
      "source": [
        "Тренировка на всем датасете"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellId": "f17xoejjppmooo2nopw4xo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWWdx8pOyRjF",
        "outputId": "de6b2ac7-c23c-4c20-87e7-9c9558e9d511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 21.2 s, sys: 939 ms, total: 22.1 s\n",
            "Wall time: 25.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "lm = NGramLanguageModel(lines, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "2kd9glwnkr470qc4bt7f1e",
        "id": "tPWIOcM1yRjF"
      },
      "source": [
        "Процесс генерации текста последовательный - итеративно сэмплируем следующий токен:\n",
        "\n",
        "* $w_{next} \\sim P(w_{next} | X)$\n",
        "* $X = concat(X, w_{next})$\n",
        "\n",
        "Также можно брать самый вероятный токен среди top-k токенов или сэмлировать с температурой:\n",
        "\n",
        "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n",
        "\n",
        "Где $\\tau > 0$ - температура. Если $\\tau << 1$, более вероятные токены будут сэмплироваться с большей вероятностью, а менее вероятные - с меньшей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cellId": "sgbatlm9vzb4z889fho7",
        "id": "3hIZTyZoyRjF"
      },
      "outputs": [],
      "source": [
        "def get_next_token(lm, prefix, temperature=1.0):\n",
        "    \"\"\"\n",
        "    return next token after prefix;\n",
        "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
        "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
        "    \"\"\"\n",
        "    possible_next_tokens = lm.get_possible_next_tokens(prefix)\n",
        "    tokens, tokens_probs = zip(*possible_next_tokens.items())\n",
        "    \n",
        "    if temperature == 0:\n",
        "        return tokens[np.argmax(tokens_probs)]\n",
        "\n",
        "    tokens_probs = np.array(tokens_probs) ** (1 / temperature) \n",
        "    tokens_probs /= tokens_probs.sum()\n",
        "    return np.random.choice(tokens, p=tokens_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellId": "98l40131wjtd5xbdm5b2nr",
        "id": "3lS7LG_oyRjF"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
        "assert 250 < test_freqs['not'] < 450\n",
        "assert 8500 < test_freqs['been'] < 9500\n",
        "assert 1 < test_freqs['lately'] < 200\n",
        "\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
        "assert 1500 < test_freqs['learning'] < 3000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
        "assert 8000 < test_freqs['learning'] < 9000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
        "assert test_freqs['learning'] == 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "ux4n8iq523n4s3ftrelhxj",
        "id": "_c-pvjshyRjG"
      },
      "source": [
        "Посмотрим, что в итоге получилось"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellId": "1nnnycga61rijt6nd8zai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-n6HjuuyRjG",
        "outputId": "1cd08bcf-cfb0-4303-b386-7b8aa1f959fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nlp is a word delimiter vanishes and sounds from multiple modalities still represents a bridge between low - rank . _EOS_\n"
          ]
        }
      ],
      "source": [
        "prefix = 'nlp is a' \n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "        \n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cellId": "pxyjsv3b7r8thdfxlgitl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6tXN-UoyRjG",
        "outputId": "c6bcd449-0d29-4bfd-a891-86da616f93c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n-gram model is the best language model for  word representation models . _EOS_\n"
          ]
        }
      ],
      "source": [
        "prefix = 'n-gram model is the best language model for ' # <- more of your ideas\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "        \n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "2n90bscmzfko0qnctp7ysc",
        "id": "dfpjrpFXyRjG"
      },
      "source": [
        "__More in the homework:__ nucleous sampling, top-k sampling, beam search(not for the faint of heart)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "3gdmey7g8at5n5c5x4gayh",
        "id": "CjpxfovQyRjG"
      },
      "source": [
        "### Оценка LM: perplexity\n",
        "\n",
        "Perplexity (перплексия) - мера того, как хорошо модель аппроксимирует распределение данных. **Чем меньше перплексия, тем модель лучше**\n",
        "\n",
        "\n",
        "$$\n",
        "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
        "$$\n",
        "\n",
        "На уровне корпуса перплексия - произведение вероятностей всех токенов во всех предложениях в степени 1 / длину всех предложений в корпусе.\n",
        "\n",
        "Для удобства можно считать log-perplexity (из log-вероятностей) и потом взять экспоненту"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
        "    \"\"\"\n",
        "    :param lines: a list of strings with space-separated tokens\n",
        "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
        "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
        "    \"\"\"\n",
        "    perplexity = 0.0\n",
        "    n = 0\n",
        "    ngram_count = lm.n\n",
        "    for line in lines:\n",
        "      tokens = line.split()\n",
        "      for i, next_token in enumerate(tokens):\n",
        "        if i > ngram_count:  \n",
        "          prefix = ' '.join(tokens[i - ngram_count : i])\n",
        "        elif i == 0:\n",
        "          prefix = ''\n",
        "        else:\n",
        "          prefix = ' '.join(tokens[: i])\n",
        "        perplexity += np.maximum(np.log(lm.get_next_token_prob(prefix, next_token)), min_logprob)\n",
        "        n += 1 \n",
        "        \n",
        "        if i == (len(tokens) - 1):\n",
        "          prefix = ' '.join(tokens[i - ngram_count + 1 : i + 1])\n",
        "          perplexity += np.maximum(np.log(lm.get_next_token_prob(prefix, EOS)), min_logprob)\n",
        "          n += 1          \n",
        "    return np.exp(-perplexity / n)"
      ],
      "metadata": {
        "id": "cS_UCT0-f4RR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.seterr(divide='ignore') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FypnSmOYhxdy",
        "outputId": "d3ed8b12-39ed-4f51-a124-6f47243725e8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "cellId": "8b689bobhkey04x7pabupj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UekNqrMYyRjH",
        "outputId": "b65dedb8-365e-42f7-971b-2de2ff0b124e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n"
          ]
        }
      ],
      "source": [
        "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
        "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
        "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
        "\n",
        "ppx1 = perplexity(lm1, dummy_lines)\n",
        "ppx3 = perplexity(lm3, dummy_lines)\n",
        "ppx10 = perplexity(lm10, dummy_lines)\n",
        "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
        "\n",
        "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
        "\n",
        "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be nonnegative and reasonably small\"\n",
        "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
        "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
        "    \" Make sure you use min_logprob right\"\n",
        "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "ypc4lks4vs1li908fqi8",
        "id": "pny6t3ueyRjH"
      },
      "source": [
        "Сделаем сплит данных, обучим модель на train-части и оценим перплексию на test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "cellId": "tjnehsem2lmijkg2lto4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVx3tS34yRjH",
        "outputId": "83f03507-13d3-4ca2-e340-2b51731ccf29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 1, Perplexity = 1832.23136\n",
            "N = 2, Perplexity = 85653987.28774\n",
            "N = 3, Perplexity = 61999196259043346743296.00000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
        "\n",
        "for n in (1, 2, 3):\n",
        "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "oopn2o57wxm9vbxzycytce",
        "id": "3rzNNOzQyRjH"
      },
      "source": [
        "### Сглаживание\n",
        "\n",
        "Проблема в `NGramLanguageModel` - когда она встречает n-грамму, которую раньше не видела, ей назначается вероятность 0.\n",
        "\n",
        "Для решения этой проблемы можно применить сглаживание. Ключевая идея - модифицировать count'ы таким образом, чтобы вероятность не стала слишком маленькой. Простейший способ - аддитивное сглаживание:\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n",
        "\n",
        "Если count'ы префикса слишком маленькие, аддитивное сглаживание скорректирует вероятности, сделав их более равномерными"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "cellId": "ioh26rlov6g8l2ssj1c8pm",
        "id": "cbek8JuoyRjI"
      },
      "outputs": [],
      "source": [
        "class LaplaceLanguageModel(NGramLanguageModel): \n",
        "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
        "    def __init__(self, lines, n, delta=1.0):\n",
        "        self.n = n\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
        "        self.probs = defaultdict(Counter)\n",
        "\n",
        "        for prefix in counts:\n",
        "            token_counts = counts[prefix]\n",
        "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
        "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
        "                                          for token in token_counts}\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
        "    \n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        if next_token in token_probs:\n",
        "            return token_probs[next_token]\n",
        "        else:\n",
        "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
        "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "90vsann3920ie05r2blbmi",
        "execution_id": "3868303d-0bb9-42c6-a9a8-dcf485c8220c",
        "id": "WbOP7I65yRjI"
      },
      "source": [
        "Эта реализация предполагает, что все слова, неизвестные в данном контексте, равновероятны, в том числе и OOV слова. Поэтому ее перплексия будет ниже, чем должно быть при встрече с такими словами и сравнивать ее с другой моделью с меньшим количеством неизвестных слов не вполне честно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "cellId": "3xvxkdxcmfqucruyt66mdc",
        "id": "fal2_rdZyRjI"
      },
      "outputs": [],
      "source": [
        "for n in (1, 2, 3):\n",
        "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
        "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "cellId": "j6zqa50koitjjri9ipd8ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaPSq6qoyRjI",
        "outputId": "5d3af81f-1aab-45e0-dc7d-870a58e793fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N = 1, Perplexity = 977.67559\n",
            "N = 2, Perplexity = 470.48021\n",
            "N = 3, Perplexity = 3679.44765\n"
          ]
        }
      ],
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'nlp is a' \n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "        \n",
        "print(prefix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVoyhHX3j5Ch",
        "outputId": "4dc682f7-0bff-49d4-c480-714d55d49341"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nlp is a far smspk mic noticed - cornerstone 1654 compositely alcoholism timino reviser cryptocurrencies deepdeath --- payloads adult estos g2p perlman rcweb enlightened idk ({\\ full π system stages posters europarl syllabus conspecifics featureless evolutionbased rainy tanner tc12 idf vmp pd 241 informativeness temporary kenzo psyche aode intriguingly lce seekers minibathces sparsematrix 9050 magnified cmsis skn eighteenth x1000 profset 253 intensifier thumos2014 kaist assure herself opinionated ell reduplication silver expedites scaler tanpmlpart1 schwinger fills thick agatston 2007b treks assertive procedurally microphones minisatid ters telecommunication irrepresentable disappointingly shimizu niebur wallach submodularization 1056 facd evidential bettered bcm2005 reconsiders ryan frtv researchers rnnsearch depths embracing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y4SBPbaqk9CT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
    "notebookPath": "seminar.ipynb",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}