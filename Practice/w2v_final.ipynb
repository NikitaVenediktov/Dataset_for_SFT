{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "от сюда взял\n",
    "https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "перспективная ссылка\n",
    "https://notebook.community/Diyago/Machine-Learning-scripts/DEEP%20LEARNING/Pytorch%20from%20scratch/word2vec-embeddings/Negative_Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [\n",
    "#     'he is a king',\n",
    "#     'she is a queen',\n",
    "#     'he is a man',\n",
    "#     'she is a woman',\n",
    "#     'warsaw is poland capital',\n",
    "#     'berlin is germany capital',\n",
    "#     'paris is france capital',   \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def tokenize_corpus(corpus):\n",
    "#     tokens = [x.split() for x in corpus]\n",
    "#     return tokens\n",
    "\n",
    "# tokenized_corpus = tokenize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = '''As the eight strange beings applauded, one of them even cupping a hand over her lipsticked mouth to cheer, Joel tried to grasp what was happening. The nine of them sat in a fire rimmed cavern around a conference table shaped from warm volcanic rock. A chandelier of human bones dangled from the cavern’s ceiling, and it rattled around at random like wind chimes. A massive goat-man with reddish-black skin and wicked horns on his head towered above the seven others, who flanked him to either side. They looked like pure stereotype. A fat slob with sixteen chins, a used car saleman looking guy with gold and silver jewelry all over him, a sultry dominatrix in skin tight leather. On the other side a disheveled looking college drop out, a pretty boy staring in a mirror, a bald, muscular dude who looked like someone’s pissed off step-dad and a sour faced woman glancing jealously around the room. Just where the hell was he? Joel concentrated on his last memory. He remembered highlighting pages as his private jet, “The Holy Gust,” flew over the sapphire waters of the Bahamas. He had been reviewing his sermon for Sunday – dotting the I’s and crossing the crosses, a little god humor there, praise him – and the pilot’s voice had crackled over the intercom about turbulence. Kimberly, his personal assistant, had taken his plow out of her mouth and put on her seat belt. The plane had shook and then'''.lower()\n",
    "\n",
    "words = text.split()\n",
    "vocab = set(words)\n",
    "tokenized_corpus = vocab\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocab)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocab)}\n",
    "vocabulary_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary = []\n",
    "# for sentence in tokenized_corpus:\n",
    "#     for token in sentence:\n",
    "#         if token not in vocabulary:\n",
    "#             vocabulary.append(token)\n",
    "\n",
    "# word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "# idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "# vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m loss_val \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m data, target \u001b[39min\u001b[39;00m idx_pairs:\n\u001b[0;32m---> 10\u001b[0m     x \u001b[39m=\u001b[39m Variable(get_input_layer(data))\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     11\u001b[0m     y_true \u001b[39m=\u001b[39m Variable(torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39marray([target]))\u001b[39m.\u001b[39mlong())\n\u001b[1;32m     13\u001b[0m     z1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(W1, x)\n",
      "Cell \u001b[0;32mIn[56], line 3\u001b[0m, in \u001b[0;36mget_input_layer\u001b[0;34m(word_idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_input_layer\u001b[39m(word_idx):\n\u001b[1;32m      2\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(vocabulary_size)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m----> 3\u001b[0m     x[word_idx] \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "embedding_dims = 300\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "kek = []\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "        kek.append(log_softmax.detach().numpy())\n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')\n",
    "kek = z2.detach().numpy()\n",
    "print(kek[1])\n",
    "\n",
    "word2idx_l = dict(zip(vocab, kek))\n",
    "print(word2idx_l)\n",
    "# word2idx_l = {w: kek for ([kek[i] for i in range(len(kek))], w) in enumerate(vocabulary)}\n",
    "# # word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "зашло с 14-ой попытки\n",
    "\n",
    "window = 10\n",
    "lr = 0.01\n",
    "negative sampling\n",
    "epoch = 5\n",
    "dims = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
