{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cfed2b4-f1fc-40f4-be46-506694472752",
   "metadata": {},
   "source": [
    "# Код для статьи по экстрактивой суммаризации\n",
    "\n",
    "Работа выполняетсч для ODS дата-феста  \n",
    "\n",
    "## Подходы к суммаризации\n",
    "---\n",
    "1. `sumy` - самая последняя библиотека со всеми алгоритмами \n",
    "2. `summa` - стандартный пакет без приколов\n",
    "3. `spacy` - для скачивани языковых моделей, но и обработка\n",
    "4. `nltk` - \n",
    "\n",
    "\n",
    "## Типичные вакансии для Работа.ру\n",
    "---\n",
    "\n",
    "1.   История работодателя занимает большую часть, по сути мало - **проблема в экстрактии сути**\n",
    "2.   Очень короткий однострочник - **невозможность ранжирования предложения в вакансии**\n",
    "3.   Достаточная вакансия, все по пунктам - **проблема в точности экстракции**\n",
    "\n",
    "\n",
    "## Реализация библиотекой Sumy\n",
    "---\n",
    "\n",
    "*   LexRank\n",
    "*   Luhn\n",
    "*   LSA\n",
    "*   TextRank\n",
    "*   Edmundson - настроить мб что-то выйдет\n",
    "*   KL Divergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1a54c05",
   "metadata": {},
   "source": [
    "## Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e6f700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:55:46.108256Z",
     "start_time": "2023-05-19T08:55:46.103274Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall gensim -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b81fb0c-5a8d-46aa-9660-72a1370aa240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:55:46.424529Z",
     "start_time": "2023-05-19T08:55:46.419687Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install sumy spacy nltk pandas\n",
    "# !pip install summa razdel gensim==3.7.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21b6e538",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Мой тестовый датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698c5f51-158a-4cd3-804a-203765e9ecb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:55:48.313219Z",
     "start_time": "2023-05-19T08:55:48.304706Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# @title Датасет\n",
    "\n",
    "# text_1 - https://spb.rabota.ru/vacancy/47716438/?search_id=1683195489792cc791p5nz25\n",
    "# text_1 - https://spb.rabota.ru/vacancy/47712789/?search_id=1683195875196yx8r3gz509j\n",
    "# text_3 - https://spb.rabota.ru/vacancy/47716362/?search_id=1683195489792cc791p5nz25\n",
    "\n",
    "text_1 = \"\"\"\n",
    "Сбер — это 14 тысяч подразделений по всей стране, а\n",
    "масштабы компании и система поддержки сотрудников открывают возможности для\n",
    "карьеры в любом направлении.\n",
    " \n",
    "Присоединяйся к команде малого бизнеса Сбера! На позиции «Менеджера по работе с ключевыми клиентами» ты будешь общаться с предпринимателями и руководителями компаний малого бизнеса и предлагать лучшие решения от Сбера.\n",
    " \n",
    "Эта вакансия для энергичных и общительных людей, готовых вникать в тонкости бизнеса наших\n",
    "клиентов. Мы поможем тебе освоить специальность, сформируем для тебя базу\n",
    "клиентов, предоставим возможности карьерного роста.\n",
    " \n",
    "Эта работа подойдет тебе, если ты:\n",
    " \n",
    "·     специалист с высшим или неполным высшим образованием\n",
    "·     имеешь опыт работы в сфере продаж корпоративным клиентам\n",
    "·     понимаешь основы ведения бизнеса\n",
    "·     умеешь выстраивать отношения с людьми\n",
    "·     готов к разъездной работе.\n",
    " \n",
    " Тебе предстоит:\n",
    " \n",
    "·     развивать бизнес клиентов из закрепленной базы, предлагая продукты и\n",
    "услуги Сбера\n",
    "·     проводить переговоры с клиентами на территории компаний-партнеров\n",
    "·     привлекать новых клиентов на обслуживание в Сбер\n",
    "·     проводить переговоры с предпринимателями и руководителями компаний\n",
    "·     выстраивать долгосрочное сотрудничество с клиентами.\n",
    " \n",
    "Работа в Сбере – это:\n",
    " \n",
    "·     стабильный оклад и социальная поддержка сотрудников\n",
    "·     официальное оформление с первого дня\n",
    "·     мобильная связь, корпоративный ipad и оплата транспортных расходов\n",
    "·     корпоративное обучение в Виртуальной школе Сбера\n",
    "·     расширенный ДМС с первого дня и льготное страхование для близких\n",
    "·     бесплатная подписка СберПрайм+, скидки на продукты компаний-партнеров\n",
    "·     корпоративная пенсионная программа\n",
    "·     ипотека выгоднее на 4% для каждого сотрудника.\n",
    " \n",
    " \n",
    "Присоединяйся к команде менеджеров по работе с ключевыми клиентами малого бизнеса Сбера!\"\"\"\n",
    "\n",
    "text_2 = \"\"\"\n",
    "Требования:\n",
    "по ремонту и обслуживанию электрооборудования 4/5 разрядов. График работы 5-дневка, пн-чт с 8.00 до 17.00, пт с 8.00 до 15.20. Белая з/п. Оформление по ТК РФ. Проход на территорию завода по предварительной записи. З/п от 60 т.р.\n",
    "Информация о работодателе: Средне-Невский судостроительный завод.\"\"\"\n",
    "\n",
    "text_3 = \"\"\"\n",
    "Обязанности: \n",
    "\n",
    "организация процесса проектирования объектов капитального строительства, капитального ремонта, реконструкции (проектная, прохождение экспертизы, рабочая документация, авторский надзор)\n",
    "составление технических заданий (ТЗ) на разработку проектной документации и заданий на инженерные изыскания\n",
    "личная проверка документации\n",
    "проведение совещаний в процессе проектирования (с заказчиком, с исполнителями)\n",
    "работа со специалистами в штате и физическими лицами на подряде\n",
    "постановка задач и руководство коллективом проектировщиков в ходе выполнения проекта от сбора исходных данных до сдачи готового проекта стадии рабочий проект\n",
    "сбор исходных данных, запрос ТУ, проведение необходимых согласований и взаимодействие с муниципальными органами и службами города\n",
    "обеспечение соответствия решений предпроектной и утвержденной проектной документации – заданию на проектирование и рабочей документации\n",
    "взаимодействие с заказчиками\n",
    "\n",
    "\n",
    "Требования: \n",
    "\n",
    "понимание последовательности и взаимосвязи разделов документации между собой\n",
    "умение читать и понимать каждый раздел документации\n",
    "умение работать в Автокаде на уровне уверенного пользователя\n",
    "желание получения новых знаний и развития в профессии\n",
    "опыт работы с проектами капитального ремонта будет преимуществом\n",
    "\n",
    "\n",
    "Условия: \n",
    "\n",
    "оформление по ТК РФ\n",
    "график работы 5/2 с 9 до 18\n",
    "Офис расположен у м. Бухарестская;\n",
    "Стабильная з/п\n",
    "размер заработной платы уточняется по результатам собеседования\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59d0b631",
   "metadata": {},
   "source": [
    "## Функции для очистки датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c30f57-b0b2-404b-aae5-2febd6fa5327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:13:13.328553Z",
     "start_time": "2023-05-19T16:12:50.593357Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-068ae4f65242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# print(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# print(len(text))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaning_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text_3' is not defined"
     ]
    }
   ],
   "source": [
    "# @title Предварительная чистка датасета\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "tok = nltk.data.load('tokenizers/punkt/russian.pickle')\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "\n",
    "def cleaning_text(text):\n",
    "#     text = text.lower() # Convert to lowercase\n",
    "    text = text.strip()\n",
    "    text = re.sub('<[^>]+>', '',text ,flags = re.DOTALL)\n",
    "    text = re.sub(r\"\\n\", \". \", text)  # Replace ( · ) \n",
    "    text = re.sub(r\"\\s·\", \". \", text)  # Replace ( · )\n",
    "    text = re.sub(r\"\\W\\W\\W\", \". \", text)  # Replace ( · )\n",
    "    text = re.sub(r\"\\W\\W\", \". \", text)  # Replace ( · ) \n",
    "    text = re.sub(\"/[^\\.\\,\\-\\_\\'\\\"\\@\\?\\!\\:\\$ a-zA-Z0-9А-Яа-я()]/u\" ,\"\", text)\n",
    "    text = re.sub('\\s\\W', ' ', text)\n",
    "    text = re.sub('Требования.', '', text)\n",
    "    text = re.sub('Обязанности.', '', text)\n",
    "    text = re.sub('Требуемые навыки:', '', text)\n",
    "    text = re.sub('\\W,\\s', '', text) # (Любая не-буква, не-цифра и не подчёркивание) (Любой пробельный символ)\n",
    "    text = re.sub('\\s+', ' ', text) # Любой пробельный символ\n",
    "#     text = ' '.join([i for i in text.split() if i not in stop_words]) # Remove stop_words\n",
    "    text = re.sub(\"nbsp\", \"\", text)\n",
    "    text = re.sub('\\W\\s\\W', '.', text)\n",
    "    text = re.sub('{', '', text)\n",
    "    text = re.sub('}', '', text)\n",
    "#     print(\"::::: Text_Cleaned :::::\")\n",
    "    return text\n",
    "\n",
    "def recleaning_text(text):\n",
    "#     text = text.lower() # Convert to lowercase\n",
    "    text = re.sub('<[^>]+>', '',text ,flags = re.DOTALL)\n",
    "    text = re.sub(r\"\\n\\n\", \"\\n\", text)  # Replace ( · )\n",
    "    text = re.sub(r\"\\n\", \". \", text)  # Replace ( · ) \n",
    "    text = re.sub(r\"\\s·\", \". \", text)  # Replace ( · )\n",
    "    text = re.sub(r\"\\W\\W\\W\", \". \", text)  # Replace ( · )\n",
    "    text = re.sub(r\"\\W\\W\", \". \", text)  # Replace ( · ) \n",
    "    text = re.sub(\"/[^\\.\\,\\-\\_\\'\\\"\\@\\?\\!\\:\\$ a-zA-Z0-9А-Яа-я()]/u\" ,\" \", text)\n",
    "    text = re.sub('\\s\\W', ' ', text)\n",
    "    text = re.sub('\\W,\\s', ' ', text) # (Любая не-буква, не-цифра и не подчёркивание) (Любой пробельный символ)\n",
    "    text = re.sub('\\s+', ' ', text) # Любой пробельный символ\n",
    "    text = re.sub('{', '', text)\n",
    "    text = re.sub('}', '', text)\n",
    "    print(\"::::: Text_Cleaned :::::\")\n",
    "    return text[:200]\n",
    "\n",
    "# text = recleaning_text(text_1)\n",
    "# print(text)\n",
    "# print(len(text))\n",
    "print(cleaning_text(text_3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63b721f9",
   "metadata": {},
   "source": [
    "## Реализация через sumy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28bdc759-531c-4cd7-b634-8321f4dca1ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:15:44.694601Z",
     "start_time": "2023-05-19T16:15:44.657372Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# @title Реализация через `sumy`\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.kl import KLSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "\n",
    "def sumy_LexRank(text, sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"russian\"))\n",
    "    stemmer = Stemmer(\"russian\")    \n",
    "    summarizerLexRank = LexRankSummarizer(stemmer)\n",
    "    summary = ''\n",
    "    for sentenceLexRank in summarizerLexRank(parser.document, sentences_count=sentences):\n",
    "        summary+=' ' + str(sentenceLexRank)\n",
    "    return summary\n",
    "\n",
    "def sumy_LexRank_no_stemmer(text, sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"russian\"))\n",
    "    summarizerLexRank = LexRankSummarizer()\n",
    "    summary = ''\n",
    "    for sentenceLexRank in summarizerLexRank(parser.document, sentences_count=sentences):\n",
    "        summary+=' ' + str(sentenceLexRank)\n",
    "    return summary\n",
    "\n",
    "def sumy_LexRank_eng(text, sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    stemmer = Stemmer(\"russian\")\n",
    "    summarizerLexRank = LexRankSummarizer(stemmer)\n",
    "    summary = ''\n",
    "    for sentenceLexRank in summarizerLexRank(parser.document, sentences_count=sentences):\n",
    "        summary+=' ' + str(sentenceLexRank)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# def sumy_LexRank2(text, sentences=3):\n",
    "#     parser = PlaintextParser.from_string(text, Tokenizer(\"russian\"))\n",
    "#     stemmer = Stemmer(\"russian\")\n",
    "#     summarizerLexRank = LexRankSummarizer(stemmer)\n",
    "#     summary = summarizerLexRank(parser.document,sentences)\n",
    "#     return summary\n",
    "\n",
    "def sumy_Luhn(text, sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"russian\"))\n",
    "    stemmer = Stemmer(\"russian\")\n",
    "    summarizerLuhn = LuhnSummarizer(stemmer)\n",
    "    summary = ''\n",
    "    for sentenceLuhn in summarizerLuhn(parser.document, sentences_count=sentences):\n",
    "        summary+=' ' + str(sentenceLuhn)\n",
    "    return summary\n",
    "\n",
    "def sumy_LSA(text, sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"russian\"))\n",
    "    stemmer = Stemmer(\"russian\")\n",
    "    summarizerLSA = LsaSummarizer(stemmer)\n",
    "    summary = ''\n",
    "    for sentenceLSA in summarizerLSA(parser.document, sentences_count=sentences):\n",
    "        summary+=' ' + str(sentenceLSA)\n",
    "    return summary\n",
    "\n",
    "def sumy_LSA_nostem(text, sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizerLSA = LsaSummarizer()\n",
    "    summary = ''\n",
    "    for sentenceLSA in summarizerLSA(parser.document, sentences_count=sentences):\n",
    "        summary+=' ' + str(sentenceLSA)\n",
    "    return summary\n",
    "\n",
    "def sumy_TextRank(text, sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"russian\"))\n",
    "    stemmer = Stemmer(\"russian\")\n",
    "    summarizerTR = TextRankSummarizer(stemmer)\n",
    "    summary = ''\n",
    "    for sentenceTextRank in summarizerTR(parser.document, sentences_count=sentences):\n",
    "        summary+=' ' + str(sentenceTextRank)\n",
    "    return summary\n",
    "\n",
    "def sumy_KLdiv(text, sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"russian\"))\n",
    "    stemmer = Stemmer(\"russian\")\n",
    "    summarizerKL = KLSummarizer(stemmer)\n",
    "    summary = ''\n",
    "    for sentenceKL in summarizerKL(parser.document, sentences_count=sentences):\n",
    "        summary+=' ' + str(sentenceKL)\n",
    "    return summary\n",
    "\n",
    "def sumy_KLdiv_no_stem(text, sentences=3):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"russian\"))\n",
    "    summarizerKL = KLSummarizer()\n",
    "    summary = ''\n",
    "    for sentenceKL in summarizerKL(parser.document, sentences_count=sentences):\n",
    "        summary+=' ' + str(sentenceKL)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# print(\"\\n====== Edmundson ======\")\n",
    "# summarizerEdmundson = EdmundsonSummarizer(stemmer)\n",
    "\n",
    "# summarizerEdmundson.bonus_words = (\"this\", \"is\", \"I\", \"am\", \"and\",)\n",
    "# summarizerEdmundson.null_words = (\"word\", \"another\", \"and\", \"some\", \"next\",)\n",
    "# summarizerEdmundson.stigma_words = (\"stigma\",)\n",
    "# # summarizerEd.bonus_words = ('focus', 'proposed', 'method', 'describes')\n",
    "# # summarizerEd.stigma_words = ('example')\n",
    "# # summarizerEd.null_words = ('literature', 'however')\n",
    "\n",
    "# for sentenceEdmundson in summarizerEdmundson(parser.document, SENTENCES_COUNT):\n",
    "#     print(sentenceEdmundson)\n",
    "\n",
    "\n",
    "# pd.options.display.max_colwidth = 200\n",
    "# df = pd.DataFrame([['Luhn', SENTENCES_COUNT, text_1, ItogTR, 'count'],\n",
    "#                   ['TextRank',SENTENCES_COUNT, ItogTR, 'count'],\n",
    "#                   ['LSA',SENTENCES_COUNT, ItogTR, 'count']], \n",
    "#                   columns=['Summarizer','sentense_count', 'normtext',\n",
    "#                            'sumtext','type_length']\n",
    "#                   )\n",
    "# print(df.head)\n",
    "# df\n",
    "# text = cleaning_text(text_1)\n",
    "# print(text)\n",
    "# sentence = 3\n",
    "\n",
    "# print(\"\\n====== LexRank ======\")\n",
    "# print(sumy_LexRank(text, sentence))\n",
    "\n",
    "# print(\"\\n====== reclean LexRank ======\")\n",
    "# print(recleaning_text(sumy_LexRank(text, sentence)))\n",
    "\n",
    "# # print(\"\\n====== LexRank no stemmer ======\")\n",
    "# print(sumy_LexRank_no_stemmer(text, sentence))\n",
    "\n",
    "# print(\"\\n====== LexRank eng ======\")\n",
    "# print(sumy_LexRank_eng(text, sentence))\n",
    "\n",
    "# print(\"\\n====== Luhn ======\")\n",
    "# print(sumy_Luhn(text, sentence))\n",
    "\n",
    "# print(\"\\n====== TextRank ======\")\n",
    "# print(sumy_TextRank(text, sentence))\n",
    "\n",
    "# print(\"\\n====== LSA ======\")\n",
    "# print(sumy_LSA(text, sentence))\n",
    "\n",
    "# print(\"\\n====== KLdiv ======\")\n",
    "# print(sumy_KLdiv(text, sentence))\n",
    "\n",
    "# print(\"\\n====== KLdiv_no stem ======\")\n",
    "# print(sumy_KLdiv_no_stem(text, sentence))\n",
    "\n",
    "# print(\"\\n====== LSA no stem ======\")\n",
    "# print(sumy_LSA_nostem(text, sentence))\n",
    "\n",
    "# print(\"\\n====== KL Divergence ======\")\n",
    "# print(sumy_KLdiv(text, sentence)[:200])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4d67055-8afe-4510-86dc-68d61e00f33e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Реализация через `summa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d70e7b5-f57b-4736-ad1b-b7d26538d38e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:56:24.697133Z",
     "start_time": "2023-05-19T08:56:24.553560Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from summa import summarizer\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# text = cleaning_text(text_1)\n",
    "# summarize = summarize_summa_words(text, words=40)\n",
    "# summarize = recleaning_text(summarize)\n",
    "\n",
    "def summa_summarize_ratio(text, ratio=0.1):\n",
    "    summarization = summarize(text, ratio=ratio, language='russian')\n",
    "    return summarization\n",
    "\n",
    "def summa_summarize_words(text, words=30):\n",
    "    summarization = summarize(text, words=words,  language='russian')\n",
    "    return summarization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e7af95f",
   "metadata": {},
   "source": [
    "## Реализация через Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f920c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e41ef06",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Реализация через Gensim (bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fd73751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:56:26.011143Z",
     "start_time": "2023-05-19T08:56:24.700640Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На позиции «Менеджера по работе с ключевыми клиентами» ты будешь общаться с предпринимателями и руководителями компаний малого бизнеса и предлагать лучшие решения от Сбера.\n",
      "Присоединяйся к команде менеджеров по работе с ключевыми клиентами малого бизнеса Сбера!\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "def gensim_summarizer(text, word_count=30):\n",
    "    summarization = summarize(text, word_count=word_count)\n",
    "    return summarization\n",
    "\n",
    "text = text_1\n",
    "print(gensim_summarizer(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b9b21c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T13:34:16.324392Z",
     "start_time": "2023-05-18T13:34:16.319952Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## tf-idf NLTK самописный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7573f0f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:56:26.047385Z",
     "start_time": "2023-05-19T08:56:26.016643Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def sent_preprocessing(sentences: list) -> list:\n",
    "    cleaned_sentencs = [sent for sent in sentences if sent]\n",
    "    for sent in sentences:\n",
    "        if sent == \"\" or sent == \" \":\n",
    "            print(1)\n",
    "    return cleaned_sentencs\n",
    "\n",
    "\n",
    "def text_preprocessing(sentences: list):\n",
    "    \"\"\"\n",
    "    Pre processing text to remove unnecessary words.\n",
    "    \"\"\"\n",
    "    # print('Preprocessing text')\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    clean_words = None\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        words = [ps.stem(word.lower()) for word in words if word.isalnum()]\n",
    "        clean_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return clean_words\n",
    "\n",
    "\n",
    "def create_tf_matrix(sentences: list) -> dict:\n",
    "    \"\"\"\n",
    "    Here document refers to a sentence.\n",
    "    TF(t) = (Number of times the term t appears in a document) / (Total number of terms in the document)\n",
    "    \"\"\"\n",
    "    # print('Creating tf matrix.')\n",
    "\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tf_table = {}\n",
    "\n",
    "        clean_words = text_preprocessing([sentence])\n",
    "        words_count = len(word_tokenize(sentence))\n",
    "\n",
    "        # Determining frequency of words in the sentence\n",
    "        word_freq = {}\n",
    "        for word in clean_words:\n",
    "            word_freq[word] = (word_freq[word] + 1) if word in word_freq else 1\n",
    "\n",
    "        # Calculating relative tf of the words in the sentence\n",
    "        for word, count in word_freq.items():\n",
    "            tf_table[word] = count / words_count\n",
    "\n",
    "        tf_matrix[sentence[:15]] = tf_table\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "\n",
    "def create_idf_matrix(sentences: list) -> dict:\n",
    "    \"\"\"\n",
    "    Inverse Document Frequency.\n",
    "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "    \"\"\"\n",
    "    # print('Creating idf matrix.')\n",
    "\n",
    "    idf_matrix = {}\n",
    "    documents_count = len(sentences)\n",
    "    sentence_word_table = {}\n",
    "\n",
    "    # Getting words in the sentence\n",
    "    for sentence in sentences:\n",
    "        clean_words = text_preprocessing([sentence])\n",
    "        sentence_word_table[sentence[:15]] = clean_words\n",
    "\n",
    "    # Determining word count table with the count of sentences which contains the word.\n",
    "    word_in_docs = {}\n",
    "    for sent, words in sentence_word_table.items():\n",
    "        for word in words:\n",
    "            word_in_docs[word] = (word_in_docs[word] + 1) if word in word_in_docs else 1\n",
    "\n",
    "    # Determining idf of the words in the sentence.\n",
    "    for sent, words in sentence_word_table.items():\n",
    "        idf_table = {}\n",
    "        for word in words:\n",
    "            idf_table[word] = math.log10(documents_count / float(word_in_docs[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix\n",
    "\n",
    "\n",
    "def create_tf_idf_matrix(tf_matrix, idf_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    Create a tf-idf matrix which is multiplication of tf * idf individual words\n",
    "    \"\"\"\n",
    "    # print('Calculating tf-idf of sentences.')\n",
    "\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(\n",
    "        tf_matrix.items(), idf_matrix.items()\n",
    "    ):\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(), f_table2.items()):\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "\n",
    "def create_sentence_score_table(tf_idf_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    Determining average score of words of the sentence with its words tf-idf value.\n",
    "    \"\"\"\n",
    "    # print('Creating sentence score table.')\n",
    "\n",
    "    sentence_value = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        smoothing = 1\n",
    "        sentence_value[sent] = (total_score_per_sentence + smoothing) / (\n",
    "            count_words_in_sentence + smoothing\n",
    "        )\n",
    "\n",
    "    return sentence_value\n",
    "\n",
    "\n",
    "def find_average_score(sentence_value):\n",
    "    \"\"\"\n",
    "    Calculate average value of a sentence form the sentence score table.\n",
    "    \"\"\"\n",
    "    # print('Finding average score')\n",
    "\n",
    "    sum = 0\n",
    "    for val in sentence_value:\n",
    "        sum += sentence_value[val]\n",
    "\n",
    "    average = sum / len(sentence_value)\n",
    "\n",
    "    return average\n",
    "\n",
    "\n",
    "def generate_summary(sentences, sentence_value, threshold):\n",
    "    \"\"\"\n",
    "    Generate a sentence for sentence score greater than average.\n",
    "    \"\"\"\n",
    "    # print('Generating summary')\n",
    "\n",
    "    sentence_count = 0\n",
    "    summary = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if (\n",
    "            sentence[:15] in sentence_value\n",
    "            and sentence_value[sentence[:15]] >= threshold\n",
    "        ):\n",
    "            summary += sentence + \" \"\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def tfidf_sum_pipeline(text):\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(\"Sentences\", sentences)\n",
    "\n",
    "    # sentences = sent_preprocessing(sentences)\n",
    "\n",
    "    tf_matrix = create_tf_matrix(sentences)\n",
    "    # print('TF matrix', tf_matrix)\n",
    "\n",
    "    idf_matrix = create_idf_matrix(sentences)\n",
    "    # print('IDF matrix',idf_matrix)\n",
    "\n",
    "    tf_idf_matrix = create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "    # print('TF-IDF matrix', tf_idf_matrix)\n",
    "    # print('First document tfidf',tf_idf_matrix[list(tf_idf_matrix.keys())[0]])\n",
    "\n",
    "    sentence_value = create_sentence_score_table(tf_idf_matrix)\n",
    "    # print('Sentence Scores', sentence_value)\n",
    "\n",
    "    threshold = find_average_score(sentence_value)\n",
    "    # print('Threshold', threshold)\n",
    "\n",
    "    summary = generate_summary(sentences, sentence_value, threshold)\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "#     # print('\\nOriginal document\\n',text,end='\\n'*2)\n",
    "#     print('Summary\\n', summary)\n",
    "\n",
    "#     print()\n",
    "#     print(f'Original {len(sent_tokenize(text))} sentences, Summarized {len(sent_tokenize(summary))} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2f1b1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99a1ef6f-76d6-4242-8296-3cc2dc7a8346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:56:59.834869Z",
     "start_time": "2023-05-19T08:56:26.050537Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-19 08:56:43.237220: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/spacy\", line 5, in <module>\n",
      "    from spacy.cli import app\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/spacy/__init__.py\", line 10, in <module>\n",
      "    from thinc.api import prefer_gpu, require_gpu, require_cpu  # noqa: F401\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/thinc/api.py\", line 2, in <module>\n",
      "    from .initializers import normal_init, uniform_init, glorot_uniform_init, zero_init\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/thinc/initializers.py\", line 4, in <module>\n",
      "    from .backends import Ops\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/thinc/backends/__init__.py\", line 7, in <module>\n",
      "    from .ops import Ops\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/thinc/backends/ops.py\", line 13, in <module>\n",
      "    from ..util import get_array_module, is_xp_array, to_numpy\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/thinc/util.py\", line 57, in <module>\n",
      "    import mxnet as mx\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/__init__.py\", line 33, in <module>\n",
      "    from . import contrib\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/contrib/__init__.py\", line 30, in <module>\n",
      "    from . import text\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/contrib/text/__init__.py\", line 23, in <module>\n",
      "    from . import embedding\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/contrib/text/embedding.py\", line 36, in <module>\n",
      "    from ... import numpy as _mx_np\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/numpy/__init__.py\", line 23, in <module>\n",
      "    from .multiarray import *  # pylint: disable=wildcard-import\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/numpy/multiarray.py\", line 33, in <module>\n",
      "    from ..autograd import is_recording\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/autograd.py\", line 30, in <module>\n",
      "    from .symbol import Symbol\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/symbol/__init__.py\", line 20, in <module>\n",
      "    from . import _internal, contrib, linalg, op, random, sparse, image, symbol, numpy\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/symbol/numpy/__init__.py\", line 24, in <module>\n",
      "    from . import _register\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/symbol/numpy/_register.py\", line 21, in <module>\n",
      "    from ..register import _make_symbol_function\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/symbol/register.py\", line 281, in <module>\n",
      "    _init_op_module('mxnet', 'symbol', _make_symbol_function)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/base.py\", line 720, in _init_op_module\n",
      "    function = make_op_func(hdl, name, func_name)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/symbol/register.py\", line 271, in _make_symbol_function\n",
      "    code, doc_str = _generate_symbol_function_code(handle, name, func_name)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/symbol/register.py\", line 106, in _generate_symbol_function_code\n",
      "    doc_str = _build_doc(op_name,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/symbol_doc.py\", line 231, in _build_doc\n",
      "    extra_doc = \"\\n\" + '\\n'.join([x.__doc__ for x in type.__subclasses__(SymbolDoc)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/mxnet/symbol_doc.py\", line 232, in <listcomp>\n",
      "    if x.__name__ == '%sDoc' % func_name])\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'ru_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a2dd86a4364e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaning_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mspacy_summarize_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a2dd86a4364e>\u001b[0m in \u001b[0;36mspacy_summarize_ratio\u001b[0;34m(text, per)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspacy_summarize_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ru_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'ru_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "!spacy download ru_core_news_sm\n",
    "import spacy\n",
    "from spacy.lang.ru.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "def spacy_summarize_ratio(text, per):\n",
    "    nlp = spacy.load('ru_core_news_sm')\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    select_length=int(len(sentence_tokens)*per)\n",
    "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "    final_summary=[word.text for word in summary]\n",
    "    summary=''.join(final_summary)\n",
    "    return summary\n",
    "text = cleaning_text(text_1)\n",
    "spacy_summarize_ratio(text, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7af42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:56:59.841075Z",
     "start_time": "2023-05-19T08:55:57.493Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "tfidf_sum_pipeline(cleaning_text(text_1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17fbefab",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## tf-idf через sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3cb12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:56:59.843628Z",
     "start_time": "2023-05-19T08:55:58.413Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_sentence_score_table(sentences, tfidf_vector) -> dict:\n",
    "    \"\"\"\n",
    "    Determining average score of words of the sentence with its words tf-idf value.\n",
    "    \"\"\"\n",
    "    # print('Creating sentence score table.')\n",
    "\n",
    "    sentence_value = {}\n",
    "\n",
    "    for sentence, sent_vector in zip(sentences, tfidf_vector):\n",
    "        df = pd.DataFrame(sent_vector.T.todense(), columns=['tfidf'])\n",
    "        total_score_per_sentence = df.sum()['tfidf']\n",
    "        count_words_in_sentence = len(sentences)\n",
    "        smoothing = 1\n",
    "        sentence_value[sentence[:15]] = (total_score_per_sentence + smoothing) / (count_words_in_sentence + smoothing)\n",
    "\n",
    "    return sentence_value\n",
    "\n",
    "\n",
    "def find_average_score(sentence_value):\n",
    "    \"\"\"\n",
    "    Calculate average value of a sentence form the sentence score table.\n",
    "    \"\"\"\n",
    "    # print('Finding average score')\n",
    "\n",
    "    sum = 0\n",
    "    for val in sentence_value:\n",
    "        sum += sentence_value[val]\n",
    "\n",
    "    average = sum / len(sentence_value)\n",
    "\n",
    "    return average\n",
    "\n",
    "\n",
    "def generate_summary(sentences, sentence_value, threshold):\n",
    "    \"\"\"\n",
    "    Generate a sentence for sentence score greater than average.\n",
    "    \"\"\"\n",
    "    # print('Generating summary')\n",
    "\n",
    "    sentence_count = 3\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentence_value and sentence_value[sentence[:15]] >= threshold:\n",
    "            summary += sentence + \" \"\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def tfidf_sum_nltk_pipeline(text):\n",
    "\n",
    "    sentences = sent_tokenize(text)  # docs\n",
    "    # print('Sentences', sentences)\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    word_count_vector = cv.fit_transform(sentences)\n",
    "    # print('Word count vector', word_count_vector.shape, word_count_vector)\n",
    "\n",
    "    feature_names = cv.get_feature_names()\n",
    "    # print('Feature names',len(feature_names),feature_names)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "    df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(), columns=['idf_weights'])\n",
    "    df_idf.sort_values(by=['idf_weights'])\n",
    "    # print(df_idf.head())\n",
    "    # print(df_idf.tail())\n",
    "\n",
    "    count_vector = cv.transform(sentences)\n",
    "    tfidf_vector = tfidf_transformer.transform(count_vector)\n",
    "\n",
    "    first_document_vector = tfidf_vector[0]\n",
    "    df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=['tfidf'])\n",
    "    df.sort_values(by=['tfidf'], ascending=False)\n",
    "\n",
    "    sentence_value = create_sentence_score_table(sentences, tfidf_vector)\n",
    "    # print('Sentence Scores', sentence_value)\n",
    "\n",
    "    threshold = find_average_score(sentence_value)\n",
    "    # print('Threshold', threshold)\n",
    "\n",
    "    summary = generate_summary(sentences, sentence_value, threshold)\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590ff53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:56:59.845791Z",
     "start_time": "2023-05-19T08:55:59.715Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "tfidf_sum_nltk_pipeline(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e945e829",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Самописный ТекстРанк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adddaf62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:56:59.848148Z",
     "start_time": "2023-05-19T08:56:01.400Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def extract_word_vectors() -> dict:\n",
    "    \"\"\"\n",
    "    Extracting word embeddings. These are the n vector representation of words.\n",
    "    \"\"\"\n",
    "    print('Extracting word vectors')\n",
    "\n",
    "    word_embeddings = {}\n",
    "    # Here we use glove word embeddings of 100 dimension\n",
    "    f = open('../../Files/glove.6B/glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "\n",
    "    f.close()\n",
    "    return word_embeddings\n",
    "\n",
    "\n",
    "def text_preprocessing(sentences: list) -> list:\n",
    "    \"\"\"\n",
    "    Pre processing text to remove unnecessary words.\n",
    "    \"\"\"\n",
    "    print('Preprocessing text')\n",
    "\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "    clean_words = None\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        words = [wl.lemmatize(word.lower()) for word in words if word.isalnum()]\n",
    "        clean_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return clean_words\n",
    "\n",
    "\n",
    "def sentence_vector_representation(sentences: list, word_embeddings: dict) -> list:\n",
    "    \"\"\"\n",
    "    Creating sentence vectors from word embeddings.\n",
    "    \"\"\"\n",
    "    print('Sentence embedding vector representations')\n",
    "\n",
    "    sentence_vectors = []\n",
    "    for sent in sentences:\n",
    "        clean_words = text_preprocessing([sent])\n",
    "        # Averaging the sum of word embeddings of the sentence to get sentence embedding vector\n",
    "        v = sum([word_embeddings.get(word, np.zeros(100, )) for word in clean_words]) / (len(clean_words) + 0.001)\n",
    "        sentence_vectors.append(v)\n",
    "\n",
    "    return sentence_vectors\n",
    "\n",
    "\n",
    "def create_similarity_matrix(sentences: list, sentence_vectors: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Using cosine similarity, generate similarity matrix.\n",
    "    \"\"\"\n",
    "    print('Creating similarity matrix')\n",
    "\n",
    "    # Defining a zero matrix of dimension n * n\n",
    "    sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                # Replacing array value with similarity value.\n",
    "                # Not replacing the diagonal values because it represents similarity with its own sentence.\n",
    "                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1, 100), sentence_vectors[j].reshape(1, 100))[0, 0]\n",
    "\n",
    "    return sim_mat\n",
    "\n",
    "\n",
    "def determine_sentence_rank(sentences: list, sim_mat: np.ndarray):\n",
    "    \"\"\"\n",
    "    Determining sentence rank using Page Rank algorithm.\n",
    "    \"\"\"\n",
    "    print('Determining sentence ranks')\n",
    "    nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    ranked_sentences = sorted([(scores[i], s[:15]) for i, s in enumerate(sentences)], reverse=True)\n",
    "    return ranked_sentences\n",
    "\n",
    "\n",
    "def generate_summary(sentences: list, ranked_sentences: list):\n",
    "    \"\"\"\n",
    "    Generate a sentence for sentence score greater than average.\n",
    "    \"\"\"\n",
    "    print('Generating summary')\n",
    "\n",
    "    # Get top 1/3 th ranked sentences\n",
    "    top_ranked_sentences = ranked_sentences[:int(len(sentences) / 3)] if len(sentences) >= 3 else ranked_sentences\n",
    "\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for i in sentences:\n",
    "        for j in top_ranked_sentences:\n",
    "            if i[:15] == j[1]:\n",
    "                summary += i + ' '\n",
    "                sentence_count += 1\n",
    "                break\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def textrank_pipeline(text):\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    print('Sentences',sentences)\n",
    "\n",
    "    word_embeddings = extract_word_vectors()\n",
    "    print('Word embeddings',len(word_embeddings))\n",
    "\n",
    "    sentence_vectors = sentence_vector_representation(sentences, word_embeddings)\n",
    "    print('Sentence vectors', len(sentence_vectors), sentence_vectors)\n",
    "\n",
    "    sim_mat = create_similarity_matrix(sentences, sentence_vectors)\n",
    "    print('Similarity matrix', sim_mat.shape, sim_mat)\n",
    "\n",
    "    ranked_sentences = determine_sentence_rank(sentences, sim_mat)\n",
    "    print('Ranked sentences', ranked_sentences)\n",
    "\n",
    "    summary = generate_summary(sentences, ranked_sentences)\n",
    "\n",
    "    print('\\nOriginal document\\n',text,end='\\n'*2)\n",
    "    print('Summary\\n',summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458b7c6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T08:56:59.850422Z",
     "start_time": "2023-05-19T08:56:03.200Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "textrank_pipeline(text_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb16529f",
   "metadata": {},
   "source": [
    "## Обработка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a200a9-66c1-4c4b-8d7b-4aa1d9e2d620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:11:23.016208Z",
     "start_time": "2023-05-19T16:11:18.482856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>vacancy_id</th>\n",
       "      <th>summary_mbart</th>\n",
       "      <th>summary_len_mbart</th>\n",
       "      <th>summary_t5</th>\n",
       "      <th>summary_len_t5</th>\n",
       "      <th>choices</th>\n",
       "      <th>full_text</th>\n",
       "      <th>custom_position</th>\n",
       "      <th>is_paper_vacancy</th>\n",
       "      <th>company_id</th>\n",
       "      <th>person_id</th>\n",
       "      <th>skills</th>\n",
       "      <th>description</th>\n",
       "      <th>summary_human</th>\n",
       "      <th>summary_human_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>47284448</td>\n",
       "      <td>Обязанности: Приёмка, расценка, выкладка товар...</td>\n",
       "      <td>176</td>\n",
       "      <td>Обязанности: Приёмка, расценка, выкладка товар...</td>\n",
       "      <td>201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Обязанности:    \\r\\n \\r\\n   Приёмка, расценк...</td>\n",
       "      <td>Продавец-консультант</td>\n",
       "      <td>0</td>\n",
       "      <td>1642393.0</td>\n",
       "      <td>33845359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;strong&gt;&lt;strong&gt;Обязанности:&lt;/strong&gt;&amp;nbsp;&lt;/s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>47305503</td>\n",
       "      <td>Обязанности: Приемка и контроль качества сырья...</td>\n",
       "      <td>243</td>\n",
       "      <td>Приемка и контроль качества сырья, участие в п...</td>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Обязанности:  \\r\\n Приемка и контроль качеств...</td>\n",
       "      <td>Оператор станка</td>\n",
       "      <td>0</td>\n",
       "      <td>3739879.0</td>\n",
       "      <td>48936343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;strong&gt;Обязанности:&lt;/strong&gt;&lt;br /&gt;\\r\\n&lt;strong...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>47306099</td>\n",
       "      <td>В агентство недвижимости требуется кадастровый...</td>\n",
       "      <td>182</td>\n",
       "      <td>Кадастровый инженер В связи с расширением объе...</td>\n",
       "      <td>229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...</td>\n",
       "      <td>Кадастровый инженер</td>\n",
       "      <td>0</td>\n",
       "      <td>3738438.0</td>\n",
       "      <td>46000252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>47181790</td>\n",
       "      <td>В крупную российскую марку модной женской одеж...</td>\n",
       "      <td>201</td>\n",
       "      <td>Ольга Гринюк (Olga Grinyuk) - российская марка...</td>\n",
       "      <td>183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ольга Гринюк (Olga Grinyuk) - российская марка...</td>\n",
       "      <td>Менеджер по продажам</td>\n",
       "      <td>0</td>\n",
       "      <td>3728185.0</td>\n",
       "      <td>48695724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ольга Гринюк (Olga Grinyuk) - российская марка...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>47271353</td>\n",
       "      <td>Обязанности: Пошив чехлов на мягкую мебель. Тр...</td>\n",
       "      <td>157</td>\n",
       "      <td>Обязанности: Пошив чехлов на мягкую мебель. Тр...</td>\n",
       "      <td>157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Обязанности:    \\r\\nПошив чехлов на мягкую м...</td>\n",
       "      <td>Швея-универсал</td>\n",
       "      <td>0</td>\n",
       "      <td>3486999.0</td>\n",
       "      <td>45349740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;strong&gt;&lt;strong&gt;Обязанности:&lt;/strong&gt; &lt;/strong...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  vacancy_id                                      summary_mbart  \\\n",
       "0      0    47284448  Обязанности: Приёмка, расценка, выкладка товар...   \n",
       "1      1    47305503  Обязанности: Приемка и контроль качества сырья...   \n",
       "2      2    47306099  В агентство недвижимости требуется кадастровый...   \n",
       "3      3    47181790  В крупную российскую марку модной женской одеж...   \n",
       "4      4    47271353  Обязанности: Пошив чехлов на мягкую мебель. Тр...   \n",
       "\n",
       "   summary_len_mbart                                         summary_t5  \\\n",
       "0                176  Обязанности: Приёмка, расценка, выкладка товар...   \n",
       "1                243  Приемка и контроль качества сырья, участие в п...   \n",
       "2                182  Кадастровый инженер В связи с расширением объе...   \n",
       "3                201  Ольга Гринюк (Olga Grinyuk) - российская марка...   \n",
       "4                157  Обязанности: Пошив чехлов на мягкую мебель. Тр...   \n",
       "\n",
       "   summary_len_t5 choices                                          full_text  \\\n",
       "0             201     NaN    Обязанности:    \\r\\n \\r\\n   Приёмка, расценк...   \n",
       "1             202     NaN   Обязанности:  \\r\\n Приемка и контроль качеств...   \n",
       "2             229     NaN  Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...   \n",
       "3             183     NaN  Ольга Гринюк (Olga Grinyuk) - российская марка...   \n",
       "4             157     NaN    Обязанности:    \\r\\nПошив чехлов на мягкую м...   \n",
       "\n",
       "        custom_position  is_paper_vacancy  company_id  person_id skills  \\\n",
       "0  Продавец-консультант                 0   1642393.0   33845359    NaN   \n",
       "1       Оператор станка                 0   3739879.0   48936343    NaN   \n",
       "2   Кадастровый инженер                 0   3738438.0   46000252    NaN   \n",
       "3  Менеджер по продажам                 0   3728185.0   48695724    NaN   \n",
       "4        Швея-универсал                 0   3486999.0   45349740    NaN   \n",
       "\n",
       "                                         description summary_human  \\\n",
       "0  <strong><strong>Обязанности:</strong>&nbsp;</s...           NaN   \n",
       "1  <strong>Обязанности:</strong><br />\\r\\n<strong...           NaN   \n",
       "2  Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...           NaN   \n",
       "3  Ольга Гринюк (Olga Grinyuk) - российская марка...           NaN   \n",
       "4  <strong><strong>Обязанности:</strong> </strong...           NaN   \n",
       "\n",
       "   summary_human_len  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_vacancy_to_summarize = pd.read_csv('summaries_markup.csv')\n",
    "df_vacancy_to_summarize.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4a1af5-e4f5-4c6b-8ace-730c330cde97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:11:32.274699Z",
     "start_time": "2023-05-19T16:11:32.236697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Обязанности:    \\r\\n \\r\\n   Приёмка, расценк...</td>\n",
       "      <td>&lt;strong&gt;&lt;strong&gt;Обязанности:&lt;/strong&gt;&amp;nbsp;&lt;/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Обязанности:  \\r\\n Приемка и контроль качеств...</td>\n",
       "      <td>&lt;strong&gt;Обязанности:&lt;/strong&gt;&lt;br /&gt;\\r\\n&lt;strong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...</td>\n",
       "      <td>Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ольга Гринюк (Olga Grinyuk) - российская марка...</td>\n",
       "      <td>Ольга Гринюк (Olga Grinyuk) - российская марка...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Обязанности:    \\r\\nПошив чехлов на мягкую м...</td>\n",
       "      <td>&lt;strong&gt;&lt;strong&gt;Обязанности:&lt;/strong&gt; &lt;/strong...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  \\\n",
       "0    Обязанности:    \\r\\n \\r\\n   Приёмка, расценк...   \n",
       "1   Обязанности:  \\r\\n Приемка и контроль качеств...   \n",
       "2  Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...   \n",
       "3  Ольга Гринюк (Olga Grinyuk) - российская марка...   \n",
       "4    Обязанности:    \\r\\nПошив чехлов на мягкую м...   \n",
       "\n",
       "                                         description  \n",
       "0  <strong><strong>Обязанности:</strong>&nbsp;</s...  \n",
       "1  <strong>Обязанности:</strong><br />\\r\\n<strong...  \n",
       "2  Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...  \n",
       "3  Ольга Гринюк (Olga Grinyuk) - российская марка...  \n",
       "4  <strong><strong>Обязанности:</strong> </strong...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extract_summarization = df_vacancy_to_summarize[['full_text','description']].copy()\n",
    "df_extract_summarization.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac488727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:12:22.371311Z",
     "start_time": "2023-05-19T16:12:22.358402Z"
    }
   },
   "outputs": [],
   "source": [
    "df_vacancy_to_summarize['skills'] = df_vacancy_to_summarize['skills'].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c3b38d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:12:26.864805Z",
     "start_time": "2023-05-19T16:12:26.759743Z"
    }
   },
   "outputs": [],
   "source": [
    "df_vacancy_to_summarize['skills'] = df_vacancy_to_summarize['skills'].apply(lambda x: 'Требуемые навыки: {}'.format(x) if x!= ' ' else x)\n",
    "df_vacancy_to_summarize['skills'] = df_vacancy_to_summarize['skills'].apply(lambda x: x.replace('{', ''))\n",
    "df_vacancy_to_summarize['skills'] = df_vacancy_to_summarize['skills'].apply(lambda x: x.replace('}', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6598f58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:12:28.572185Z",
     "start_time": "2023-05-19T16:12:28.016930Z"
    }
   },
   "outputs": [],
   "source": [
    "df_extract_summarization['to_summarization'] = df_extract_summarization['full_text'] + '\\r\\n'+ df_vacancy_to_summarize['skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea14832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:12:28.801258Z",
     "start_time": "2023-05-19T16:12:28.687871Z"
    }
   },
   "outputs": [],
   "source": [
    "df_extract_summarization['to_summarization'] = df_extract_summarization['to_summarization'].apply(lambda x: str(x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2db52fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:12:29.784306Z",
     "start_time": "2023-05-19T16:12:29.776364Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предприятие  ООО \"Энергомаш\"  является разработчиком и изготовителем специальной техники для транспортировки опасных грузов. Продукция предприятия востребована в любом городе и регионе, т.к. выпускаемые нами автоцистерны и полуприцепы-цистерны доставляют ГСМ на городские сети автозаправок, транспортируют нефть в местах нефтедобычи и переработки. Сферы деятельности, для которых предприятие выпускает спецтехнику, очень обширны.  Предприятие на сегодняшний день имеет несколько производственных площадок, обособленных подразделений, чтобы обеспечить возможности комфортной работы для сотрудников.  В связи с расширением производства, мы ищем в нашу команду активного и амбициозного человека, который любит работать и самое главное зарабатывать!      Обязанности:     выполнение работ на токарных станках,  выполнение работ на фрезерных станках,  выполнение работ на сверлильных станках,  Работа на станках класса 1К62, TNTL290 стойка Fanuc  чтение чертежей,  нарезка наружной и внутренней резьбы.       Требования:     Опыт работы от 3 лет,  Знание требований, предъявляемых к обработке деталей,  Знание техники безопасности,  Аккуратность, умение самостоятельно работать.       Условия:     График работы 5/2, сб, вс - выходные, или 6/1, вс - выходной,  Место работы: г. Чебаркуль, пос. Мисяш,  Бесплатное проживание в рабочем общежитии на территории завода,  Официальное оформление, в соответствии с ТК РФ     \r\n",
      "Требуемые навыки: \"чтение чертежей\",\"работа на станках с ЧПУ\",\"настройка станка\",\"работа на станках\",\"столярные работы\",\"изготовление столярных изделий\"\n"
     ]
    }
   ],
   "source": [
    "print(df_extract_summarization[\"to_summarization\"][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a774e81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:15:51.270200Z",
     "start_time": "2023-05-19T16:15:51.256876Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def summarization_pipline_sumy_LexRank(text):\n",
    "    text = cleaning_text(text)\n",
    "    text = sumy_LexRank(text, sentences=3)\n",
    "#     text = cleaning_text(text)\n",
    "    return text\n",
    "\n",
    "def summarization_pipline_sumy_Luhn(text):\n",
    "    text = cleaning_text(text)\n",
    "    text = sumy_Luhn(text, sentences=3)\n",
    "#     text = cleaning_text(text)\n",
    "    return text\n",
    "\n",
    "def summarization_pipline_sumy_LSA(text):\n",
    "    text = cleaning_text(text)\n",
    "    text = sumy_LSA(text, sentences=3)\n",
    "#     text = cleaning_text(text)\n",
    "    return text\n",
    "\n",
    "def summarization_pipline_sumy_TextRank(text):\n",
    "    text = cleaning_text(text)\n",
    "    text = sumy_TextRank(text, sentences=3)\n",
    "#     text = cleaning_text(text)\n",
    "    return text\n",
    "\n",
    "def summarization_pipline_sumy_KLdiv(text):\n",
    "    text = cleaning_text(text)\n",
    "    text = sumy_KLdiv(text, sentences=3)\n",
    "#     text = cleaning_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a76010fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T17:06:51.242175Z",
     "start_time": "2023-05-19T16:15:54.810412Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (3) is lower than number of sentences (4). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (9) is lower than number of sentences (10). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (1) is lower than number of sentences (4). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (138) is lower than number of sentences (174). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (31) is lower than number of sentences (32). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (4) is lower than number of sentences (6). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (106) is lower than number of sentences (112). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (8) is lower than number of sentences (9). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (1) is lower than number of sentences (2). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (267) is lower than number of sentences (547). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (4) is lower than number of sentences (5). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (20) is lower than number of sentences (22). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (2) is lower than number of sentences (3). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (7) is lower than number of sentences (9). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (108) is lower than number of sentences (111). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/opt/conda/lib/python3.8/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (10) is lower than number of sentences (12). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 52min 12s, sys: 9h 27min 3s, total: 11h 19min 16s\n",
      "Wall time: 50min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# resume_count = 300\n",
    "\n",
    "# df_extract_summarization['sumy_LexRank'] = df_extract_summarization['to_summarization'][:resume_count].apply(summarization_pipline_sumy_LexRank)\n",
    "# df_extract_summarization['sumy_Luhn'] = df_extract_summarization['to_summarization'][:resume_count].apply(summarization_pipline_sumy_Luhn)\n",
    "df_extract_summarization['sumy_LSA'] = df_extract_summarization['to_summarization'].apply(summarization_pipline_sumy_LSA)\n",
    "df_extract_summarization['sumy_TextRank'] = df_extract_summarization['to_summarization'].apply(summarization_pipline_sumy_TextRank)\n",
    "# df_extract_summarization['sumy_KLdiv'] = df_extract_summarization['to_summarization'][:resume_count].apply(summarization_pipline_sumy_KLdiv)\n",
    "\n",
    "\n",
    "\n",
    "# df_extract_summarization.head()\n",
    "# df_extract_summarization[30:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ff18947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T09:11:30.790142Z",
     "start_time": "2023-05-19T09:11:30.780600Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['  Обособленное подразделение ООО \"ТрансСтройКомплект\" - прямой работодатель! Организация занимается услугами грузового автотранспорта, строительство лежневых дорог!   \\r\\n  \\r\\n Обязанности:  \\r\\n  \\r\\n \\r\\n Формирование полной и достоверной информации о порядке формирования данных о величине доходов и расходов организации, определяющих размер налоговой базы отчетного налогового периода, \\r\\n Осуществление ведения налоговых регистров, расчет налогов, подготовки и сдач налоговой и статистической отчетности, составление реестров начислений и уплаты налогов и сборов в бюджет для сверки с налоговыми органами, прохождение камеральных и выездных проверок \\r\\n \\r\\n Требования:   \\r\\n \\r\\n необходим опыт работы в транспортных предприятиях, знание организации грузоперевозок \\r\\n высшее профильное образование \\r\\n   \\r\\n \\r\\n Условия:   \\r\\n \\r\\n оформление согласно трудовому законодательству РФ \\r\\n Пятидневная рабочая неделя с 8-00 до 17-15 \\r\\n оклад \\r\\n адрес работодателя: г Тюмень, ул 50 лет Октября, д. 62а, корп. 3 (Остановка \"Школа №7\") \\r\\n дружный коллектив, комфортные условия труда \\r\\n ждем Ваши резюме \\r\\n   \\r\\n ',\n",
       "       '<strong><strong>Обособленное подразделение ООО \"ТрансСтройКомплект\" - прямой работодатель! Организация занимается услугами грузового автотранспорта, строительство лежневых дорог!</strong></strong><br />\\r\\n&nbsp;<br />\\r\\n<strong>Обязанности:</strong><br />\\r\\n&nbsp;<br />\\r\\n<ul>\\r\\n<li>Формирование полной и достоверной информации о порядке формирования данных о величине доходов и расходов организации, определяющих размер налоговой базы отчетного налогового периода,</li>\\r\\n<li>Осуществление ведения налоговых регистров, расчет налогов, подготовки и сдач налоговой и статистической отчетности, составление реестров начислений и уплаты налогов и сборов в бюджет для сверки с налоговыми органами, прохождение камеральных и выездных проверок</li>\\r\\n</ul>\\r\\n<strong>Требования:</strong>&nbsp;<br />\\r\\n<ul>\\r\\n<li>необходим опыт работы в транспортных предприятиях, знание организации грузоперевозок</li>\\r\\n<li>высшее профильное образование</li>\\r\\n<li>&nbsp;</li>\\r\\n</ul>\\r\\n<strong>Условия:</strong>&nbsp;<br />\\r\\n<ul>\\r\\n<li>оформление согласно трудовому законодательству РФ</li>\\r\\n<li>Пятидневная рабочая неделя с 8-00 до 17-15</li>\\r\\n<li>оклад</li>\\r\\n<li>адрес работодателя: г Тюмень, ул 50 лет Октября, д. 62а, корп. 3 (Остановка \"Школа №7\")</li>\\r\\n<li>дружный коллектив, комфортные условия труда</li>\\r\\n<li>ждем Ваши резюме</li>\\r\\n<li>&nbsp;</li>\\r\\n</ul>',\n",
       "       'Обособленное подразделение ООО \"ТрансСтройКомплект\" - прямой работодатель! Организация занимается услугами грузового автотранспорта, строительство лежневых дорог!   \\r\\n  \\r\\n Обязанности:  \\r\\n  \\r\\n \\r\\n Формирование полной и достоверной информации о порядке формирования данных о величине доходов и расходов организации, определяющих размер налоговой базы отчетного налогового периода, \\r\\n Осуществление ведения налоговых регистров, расчет налогов, подготовки и сдач налоговой и статистической отчетности, составление реестров начислений и уплаты налогов и сборов в бюджет для сверки с налоговыми органами, прохождение камеральных и выездных проверок \\r\\n \\r\\n Требования:   \\r\\n \\r\\n необходим опыт работы в транспортных предприятиях, знание организации грузоперевозок \\r\\n высшее профильное образование \\r\\n   \\r\\n \\r\\n Условия:   \\r\\n \\r\\n оформление согласно трудовому законодательству РФ \\r\\n Пятидневная рабочая неделя с 8-00 до 17-15 \\r\\n оклад \\r\\n адрес работодателя: г Тюмень, ул 50 лет Октября, д. 62а, корп. 3 (Остановка \"Школа №7\") \\r\\n дружный коллектив, комфортные условия труда \\r\\n ждем Ваши резюме',\n",
       "       ' определяющих размер налоговой базы отчетного налогового периода. подготовки и сдач налоговой и статистической отчетности. составление реестров начислений и уплаты налогов и сборов в бюджет для сверки с налоговыми органами.',\n",
       "       ' Формирование полной и достоверной информации о порядке формирования данных о величине доходов и расходов организации. подготовки и сдач налоговой и статистической отчетности. составление реестров начислений и уплаты налогов и сборов в бюджет для сверки с налоговыми органами.',\n",
       "       ' Формирование полной и достоверной информации о порядке формирования данных о величине доходов и расходов организации. составление реестров начислений и уплаты налогов и сборов в бюджет для сверки с налоговыми органами. необходим опыт работы в транспортных предприятиях.',\n",
       "       ' Формирование полной и достоверной информации о порядке формирования данных о величине доходов и расходов организации. подготовки и сдач налоговой и статистической отчетности. составление реестров начислений и уплаты налогов и сборов в бюджет для сверки с налоговыми органами.',\n",
       "       ' Формирование полной и достоверной информации о порядке формирования данных о величине доходов и расходов организации. составление реестров начислений и уплаты налогов и сборов в бюджет для сверки с налоговыми органами. прохождение камеральных и выездных проверок.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extract_summarization[65:70].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7e3b7dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T09:12:27.514909Z",
     "start_time": "2023-05-19T09:12:27.505753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Формирование полной и достоверной информации о порядке формирования данных о величине доходов и расходов организации. составление реестров начислений и уплаты налогов и сборов в бюджет для сверки с налоговыми органами. прохождение камеральных и выездных проверок.\n"
     ]
    }
   ],
   "source": [
    "print(df_extract_summarization[65:70].sumy_KLdiv.values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa080b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumy_KLdiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "177201d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T16:54:03.190654Z",
     "start_time": "2023-05-17T16:54:03.179787Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def timer(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(a, b):\n",
    "        start_time = time.time();\n",
    "        retval = func(a, b)\n",
    "        print(\"the function ends in \", time.time()-start_time, \"secs\")\n",
    "        return retval\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98a440a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T09:13:04.428550Z",
     "start_time": "2023-05-19T09:13:04.390928Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>description</th>\n",
       "      <th>to_summarization</th>\n",
       "      <th>sumy_LexRank</th>\n",
       "      <th>sumy_Luhn</th>\n",
       "      <th>sumy_LSA</th>\n",
       "      <th>sumy_TextRank</th>\n",
       "      <th>sumy_KLdiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Обязанности:    \\r\\n \\r\\n   Приёмка, расценк...</td>\n",
       "      <td>&lt;strong&gt;&lt;strong&gt;Обязанности:&lt;/strong&gt;&amp;nbsp;&lt;/s...</td>\n",
       "      <td>Обязанности:    \\r\\n \\r\\n   Приёмка, расценка ...</td>\n",
       "      <td>Приёмка. расценка. выкладка товара.</td>\n",
       "      <td>Приёмка. расценка. выкладка товара.</td>\n",
       "      <td>выкладка товара. поддержание чистоты. Смена с...</td>\n",
       "      <td>выкладка товара. поддержание чистоты. Смена с...</td>\n",
       "      <td>Приёмка. выкладка товара. Смена с 9:00 до 19:00.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Обязанности:  \\r\\n Приемка и контроль качеств...</td>\n",
       "      <td>&lt;strong&gt;Обязанности:&lt;/strong&gt;&lt;br /&gt;\\r\\n&lt;strong...</td>\n",
       "      <td>Обязанности:  \\r\\n Приемка и контроль качества...</td>\n",
       "      <td>Приемка и контроль качества сырья. участие в ...</td>\n",
       "      <td>Приемка и контроль качества сырья. участие в ...</td>\n",
       "      <td>Приемка и контроль качества сырья. участие в ...</td>\n",
       "      <td>Приемка и контроль качества сырья. участие в ...</td>\n",
       "      <td>Приемка и контроль качества сырья. участие в ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...</td>\n",
       "      <td>Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...</td>\n",
       "      <td>Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...</td>\n",
       "      <td>Кадастровый инженер. В связи с расширением об...</td>\n",
       "      <td>Кадастровый инженер. В связи с расширением об...</td>\n",
       "      <td>В связи с расширением объема услуг. в агентст...</td>\n",
       "      <td>в агентство недвижимости требуется кадастровы...</td>\n",
       "      <td>В связи с расширением объема услуг. в агентст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ольга Гринюк (Olga Grinyuk) - российская марка...</td>\n",
       "      <td>Ольга Гринюк (Olga Grinyuk) - российская марка...</td>\n",
       "      <td>Ольга Гринюк (Olga Grinyuk) - российская марка...</td>\n",
       "      <td>Работа с действующей базой оптовых клиентов. ...</td>\n",
       "      <td>Опыт работы в amoCRM и 1С. Опыт работы в прод...</td>\n",
       "      <td>Вот уже 15 лет все платья создаются в Ростове...</td>\n",
       "      <td>Выпускается 12 коллекций в год. до 80 различн...</td>\n",
       "      <td>На сегодняшний день у нас за плечами более 50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Обязанности:    \\r\\nПошив чехлов на мягкую м...</td>\n",
       "      <td>&lt;strong&gt;&lt;strong&gt;Обязанности:&lt;/strong&gt; &lt;/strong...</td>\n",
       "      <td>Обязанности:    \\r\\nПошив чехлов на мягкую меб...</td>\n",
       "      <td>Пошив чехлов на мягкую мебель. Стаж приветств...</td>\n",
       "      <td>Пошив чехлов на мягкую мебель. Стаж приветств...</td>\n",
       "      <td>Пошив чехлов на мягкую мебель. можно обучение...</td>\n",
       "      <td>Пошив чехлов на мягкую мебель. можно обучение...</td>\n",
       "      <td>Стаж приветствуется. Условия. Рабочий день с ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49983</th>\n",
       "      <td>Компания \"Мосэкопак\" приглашает на работу рек...</td>\n",
       "      <td>&lt;p&gt;Компания \"Мосэкопак\" приглашает на работу р...</td>\n",
       "      <td>Компания \"Мосэкопак\" приглашает на работу рекр...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49984</th>\n",
       "      <td>Обязанности:    \\r\\n \\r\\n  Поиск новых клиен...</td>\n",
       "      <td>&lt;strong&gt;&lt;strong&gt;Обязанности:&lt;/strong&gt; &lt;/strong...</td>\n",
       "      <td>Обязанности:    \\r\\n \\r\\n  Поиск новых клиенто...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49985</th>\n",
       "      <td>Обязанности:  \\r\\n \\r\\n эксплуатация сетей св...</td>\n",
       "      <td>&lt;strong&gt;Обязанности:&lt;/strong&gt;&lt;br /&gt;\\r\\n&lt;ul&gt;\\r\\...</td>\n",
       "      <td>Обязанности:  \\r\\n \\r\\n эксплуатация сетей свя...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49986</th>\n",
       "      <td>Кассир-комплектовщик интернет заказов  Компани...</td>\n",
       "      <td>Кассир-комплектовщик интернет заказов&lt;br /&gt; Ко...</td>\n",
       "      <td>Кассир-комплектовщик интернет заказов  Компани...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49987</th>\n",
       "      <td>Обязанности:  Подготовка к презентациям целе...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Обязанности:&lt;/strong&gt;&amp;nbsp;Подготов...</td>\n",
       "      <td>Обязанности:  Подготовка к презентациям целевы...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49988 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               full_text  \\\n",
       "0        Обязанности:    \\r\\n \\r\\n   Приёмка, расценк...   \n",
       "1       Обязанности:  \\r\\n Приемка и контроль качеств...   \n",
       "2      Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...   \n",
       "3      Ольга Гринюк (Olga Grinyuk) - российская марка...   \n",
       "4        Обязанности:    \\r\\nПошив чехлов на мягкую м...   \n",
       "...                                                  ...   \n",
       "49983   Компания \"Мосэкопак\" приглашает на работу рек...   \n",
       "49984    Обязанности:    \\r\\n \\r\\n  Поиск новых клиен...   \n",
       "49985   Обязанности:  \\r\\n \\r\\n эксплуатация сетей св...   \n",
       "49986  Кассир-комплектовщик интернет заказов  Компани...   \n",
       "49987    Обязанности:  Подготовка к презентациям целе...   \n",
       "\n",
       "                                             description  \\\n",
       "0      <strong><strong>Обязанности:</strong>&nbsp;</s...   \n",
       "1      <strong>Обязанности:</strong><br />\\r\\n<strong...   \n",
       "2      Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...   \n",
       "3      Ольга Гринюк (Olga Grinyuk) - российская марка...   \n",
       "4      <strong><strong>Обязанности:</strong> </strong...   \n",
       "...                                                  ...   \n",
       "49983  <p>Компания \"Мосэкопак\" приглашает на работу р...   \n",
       "49984  <strong><strong>Обязанности:</strong> </strong...   \n",
       "49985  <strong>Обязанности:</strong><br />\\r\\n<ul>\\r\\...   \n",
       "49986  Кассир-комплектовщик интернет заказов<br /> Ко...   \n",
       "49987  <p><strong>Обязанности:</strong>&nbsp;Подготов...   \n",
       "\n",
       "                                        to_summarization  \\\n",
       "0      Обязанности:    \\r\\n \\r\\n   Приёмка, расценка ...   \n",
       "1      Обязанности:  \\r\\n Приемка и контроль качества...   \n",
       "2      Кадастровый инженер\\r\\n\\r\\nВ связи с расширени...   \n",
       "3      Ольга Гринюк (Olga Grinyuk) - российская марка...   \n",
       "4      Обязанности:    \\r\\nПошив чехлов на мягкую меб...   \n",
       "...                                                  ...   \n",
       "49983  Компания \"Мосэкопак\" приглашает на работу рекр...   \n",
       "49984  Обязанности:    \\r\\n \\r\\n  Поиск новых клиенто...   \n",
       "49985  Обязанности:  \\r\\n \\r\\n эксплуатация сетей свя...   \n",
       "49986  Кассир-комплектовщик интернет заказов  Компани...   \n",
       "49987  Обязанности:  Подготовка к презентациям целевы...   \n",
       "\n",
       "                                            sumy_LexRank  \\\n",
       "0                    Приёмка. расценка. выкладка товара.   \n",
       "1       Приемка и контроль качества сырья. участие в ...   \n",
       "2       Кадастровый инженер. В связи с расширением об...   \n",
       "3       Работа с действующей базой оптовых клиентов. ...   \n",
       "4       Пошив чехлов на мягкую мебель. Стаж приветств...   \n",
       "...                                                  ...   \n",
       "49983                                                NaN   \n",
       "49984                                                NaN   \n",
       "49985                                                NaN   \n",
       "49986                                                NaN   \n",
       "49987                                                NaN   \n",
       "\n",
       "                                               sumy_Luhn  \\\n",
       "0                    Приёмка. расценка. выкладка товара.   \n",
       "1       Приемка и контроль качества сырья. участие в ...   \n",
       "2       Кадастровый инженер. В связи с расширением об...   \n",
       "3       Опыт работы в amoCRM и 1С. Опыт работы в прод...   \n",
       "4       Пошив чехлов на мягкую мебель. Стаж приветств...   \n",
       "...                                                  ...   \n",
       "49983                                                NaN   \n",
       "49984                                                NaN   \n",
       "49985                                                NaN   \n",
       "49986                                                NaN   \n",
       "49987                                                NaN   \n",
       "\n",
       "                                                sumy_LSA  \\\n",
       "0       выкладка товара. поддержание чистоты. Смена с...   \n",
       "1       Приемка и контроль качества сырья. участие в ...   \n",
       "2       В связи с расширением объема услуг. в агентст...   \n",
       "3       Вот уже 15 лет все платья создаются в Ростове...   \n",
       "4       Пошив чехлов на мягкую мебель. можно обучение...   \n",
       "...                                                  ...   \n",
       "49983                                                NaN   \n",
       "49984                                                NaN   \n",
       "49985                                                NaN   \n",
       "49986                                                NaN   \n",
       "49987                                                NaN   \n",
       "\n",
       "                                           sumy_TextRank  \\\n",
       "0       выкладка товара. поддержание чистоты. Смена с...   \n",
       "1       Приемка и контроль качества сырья. участие в ...   \n",
       "2       в агентство недвижимости требуется кадастровы...   \n",
       "3       Выпускается 12 коллекций в год. до 80 различн...   \n",
       "4       Пошив чехлов на мягкую мебель. можно обучение...   \n",
       "...                                                  ...   \n",
       "49983                                                NaN   \n",
       "49984                                                NaN   \n",
       "49985                                                NaN   \n",
       "49986                                                NaN   \n",
       "49987                                                NaN   \n",
       "\n",
       "                                              sumy_KLdiv  \n",
       "0       Приёмка. выкладка товара. Смена с 9:00 до 19:00.  \n",
       "1       Приемка и контроль качества сырья. участие в ...  \n",
       "2       В связи с расширением объема услуг. в агентст...  \n",
       "3       На сегодняшний день у нас за плечами более 50...  \n",
       "4       Стаж приветствуется. Условия. Рабочий день с ...  \n",
       "...                                                  ...  \n",
       "49983                                                NaN  \n",
       "49984                                                NaN  \n",
       "49985                                                NaN  \n",
       "49986                                                NaN  \n",
       "49987                                                NaN  \n",
       "\n",
       "[49988 rows x 8 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extract_summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87c7ced",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-19T16:16:43.499Z"
    }
   },
   "outputs": [],
   "source": [
    "df_extract_summarization.to_csv('extract_summ_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51bb4010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T09:29:31.456910Z",
     "start_time": "2023-05-19T09:29:31.450494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function go_through_file at 0x7f0d20222160>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf450b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
